ROOTCORE_FAX
============
Example Tutorial for ROOTCORE Dijet Analysis running on “atlasconnect”

login to “atlasconnect”
ssh -Y username@login.atlas.ci-connect.net

Introduction

The following illustrates how to use the SmallNtupleMakerReader RootCore package written for the 2012 full dataset dijet mass resonances analysis.

code: https://svnweb.cern.ch/trac/atlasphys/browser/Physics/Exotic/Analysis/Dijet/SmallNtupleMakerReader


A) Run Interactively Over small dataset:
Step 0
To run locally you need to dq2-get a sample. You have to first setup dq2-get.
export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase
 source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh
localSetupDQ2Client

dq2-get -f  one_such_file.root data12_8TeV.00213431.physics_JetTauEtmiss.merge.NTUP_SLIMSMQCD.f482_m1238_p1344_p1345_p1380/

Create a file.list text file with the list of files contained in the sample:
ls       ~/data12_8TeV.00213431.physics_JetTauEtmiss.merge.NTUP_SLIMSMQCD.f482_m1238_p1344_p1345_p1380/*.root* > ~/data/file.list

Step 1
Setup Root: Whether on SLC5 or on SLC6

vi setuproot.sh
#!/bin/bash
export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase

source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh --quiet

export REV=`cat /etc/redhat-release | sed s/.*release\ // | sed "s/\\..*//" `

echo "The version of SLC on this node is=" $REV

if [[ $REV -eq 5 ]]; then

        echo "SLC version is    "$REV

        localSetupROOT  5.34.10-i686-slc5-gcc43-opt --skipConfirm

 else

       echo "SLC version is    "$REV

        localSetupROOT  5.34.10-x86_64-slc6-gcc47-opt --skipConfirm

fi






Step 2
Checkout the package from SVN:

export SVNPHYS=svn+ssh://svn.cern.ch/reps/atlasphys/
svn co $SVNPHYS/Physics/Exotic/Analysis/Dijet/SmallNtupleMakerReader/DijetMassResonances/trunk DijetMassResonances

Step 3
Step 3a
cd DijetMassResonances
source setup-DijetMassResonances.sh
This scripts looks for the packages needed to run the analysis and if not found it automatically downloads them. After downloading missing packages the scripts compiles the DijetMassResonances package. 

Step 3b
If this is not the first time you are using the code you have first to setup the RootCore environment:
cd ~/SmallNtupleMakerReader
source RootCore/scripts/setup.sh
Now move to step 3c.
Step 3c
Compile the code:
cd ~/SmallNtupleMakerReader
RootCore/scripts/clean.sh
RootCore/scripts/compile.sh


Step 4
What will the code do?

The common base class, ExoticDijetAnalysis, contains functionalities to:
start from a NTUP_XXYY (e.g. NTUP_SLIMSMQCD) and write out a smaller slimmed/skimmed ntuple containing events selected by GRL, trigger and TileTripReader with only branches and decoration branches (4-momentum variables for calibrated jets and cleaning flags). 
reread the slimmed/skimmed ntuple above (called small ntuple in some places in the code and below), perform pseudoexperiments and get the final plots. This part still has to be defined depending on what the statistical analysis code will require in input.

The classes are steered by small executables that can be found in the util directory.

Steering executables that have been implemented, status and short description

createSmallNtupleForDijetMassResonances: used with data/test.config generates a small ntuple using the basic event selection of the cutflow , including trigger selection with prescales and jet recalibration. 


doDijetMassResonances: reads back the small ntuple generated by createSmallNtupleForDijetMassResonances and finishes up the dijet selection, producing the final mjj plot and some control distributions. needs a new config file for re-reading

Run locally
The code takes parameters in input, just run it empty to see what these are:
cd ~/SmallNtupleMakerReader/DijetMassResonances/
createSmallNtupleForDijetMassResonances
The parameters are:
list: input file list.
name: name of the output root file.
config: config file with input parameters.

To run the analysis sample (example of createSmallNtupleForDijetMassResonances):
createSmallNtupleForDijetMassResonances --list ~/data/file.list --name results --config data/test.config
This will produce the output “results.root” containing all the info and dijet invariant mass.


B)  Now: The same script/package needs to be run over huge dataset using Condor Batch System of “ATLASCONNECT” 

Now, Instead of downloading the huge dataset, the dataset can be accessed through FAXBOX through network.

ACCESS INPUTDATA THROUGH FAXBOX

Use following command to know LFN of the dataset.

Setting up the FAX environment is done using a package from the localSetupFAX :
     export ATLAS_LOCAL_ROOT_BASE=/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase
     source ${ATLAS_LOCAL_ROOT_BASE}/user/atlasLocalSetup.sh
     localSetupFAX
A FAX access endpoint is automatically setup for you. You can find it in the environment variable STORAGEPREFIX. Create the usual grid proxy:
    voms-proxy-init -voms atlas

Use tool fax-get-gLFNs:
fax-get-gLFNs  input_dataset

for e.g In my case: I am using input dataset “data12_8TeV.00213431.physics_JetTauEtmiss.merge.NTUP_SLIMSMQCD.f482_m1238_p1344_p1345_p1380/”




Store gLFN path of the files in dataset:
fax-get-gLFNs  data12_8TeV.00213431.physics_JetTauEtmiss.merge.NTUP_SLIMSMQCD.f482_m1238_p1344_p1345_p1380/ >inputFileListLarge

Now: I have a dijet analysis package based on RootCore.
RootCore is a package that helps developers build packages that work standalone (outside Athena). It works both for packages that are root-only and for packages that work with both, Athena and root-only.
Now, we need all the packages at the node where jobs gets executed. One way is to send the tar file “dijet.tgz” alongwith the jobs. This can be done using “ transfer_input_files” in requirement.
transfer_input_files = dijet.tgz
We also have to Divide the input filelist into “n” parts; n is the number of jobs submitted. This can be done in small script in shell file:

echo "Now dividing input file lists"

echo 'job:' $1, 'from:' $2

files=$(wc -l < inputFileListLarge)

echo 'input files:' $files

awk -v jo=$1 -v totjobs=$2 -v len=$files 'BEGIN {slice = len/totjobs; start = jo*slice; end = (jo+1)*slice;} NR > start && NR <= end {print}' inputFileListLarge > inputFileList

cat inputFileList



Upto Now, we have prepared for the inputdataset to be accessed from grid and divided into number of subjobs.
To access from grid, one needs authentication. This is little tricky. Your grid credentials need to be given to batch jobs. You will need to find your grid proxy file and to copy it to batch machines.

When you do: 
voms-proxy-init -voms atlas

You get proxy  X509up_xxxx in /tmp dir.

 Set the variable in your login/shell script (named SmallD3PD.sh)
export X509_USER_PROXY=x509up_u55261
which modifies to location of the grid proxy and tells to everybody else where to look for it. One can send this “x509up_u55261”  alongwith job to the execution node using above  transfer_input_files” in requirement.
So in total:  We are sending three files: One package tarfile, one grid proxy and one inputfile alongwith job into execution node.
transfer_input_files = dijet.tgz,/tmp/x509up_u55261,inputFileListLarge


Final Requirement file called SmallD3PD.sub:
Jobs = 25
   *******Number of jobs to split 
executable     = SmallD3PD.sh
  ********Executable shell script
output         = output/SmallD3PD.out.$(Process)
  *******Outfile 
error          = error/SmallD3PD.error.$(Process)
  *******Error file 
log            = log/SmallD3PD.log.$(Process)
 ********logfile 
arguments = $(Process) $(Jobs)

should_transfer_files = YES
  ****Transfer files between submission node and execution node
when_to_transfer_output = ON_EXIT
 
transfer_input_files = dijet.tgz,/tmp/x509up_u55261,inputFileListLarge
  ***package/proxy/inputfile 
transfer_output_files = /tmp/results$(Process).root
 
+ProjectName = "atlas-org-fresnostate"
 ****Needs to define to identify you
queue $(Jobs)


Now, in SmallD3PD.sh, 
First we need to setup root, then untar the ROOTCORE Package and compile it and run the executable over the above defined inputfile using Grid Proxy and get the final output.
